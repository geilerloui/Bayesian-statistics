---
title: "Logistic_regression"
author: "louis"
date: "31 août 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lesson 9: Logistic regression

### Lesson 9.2

#### Data

For an example of logistic regression, we'll use the urine data set from the boot package in R. The response variable is r, which takes on values of 0 or 1. We will remove some rows from the data set which contain missing values.

```{r}
library("boot")
data("urine")
# ? urine: permet d'avoir plus d'info sur les données
head(urine)
```

```{r }
dat = na.omit(urine)
```

Let's look at pairwise scatter plots of the seven variables.
```{r }
pairs(dat)
```
One thing that stands out is that several of these variables are strongly correlated with one another. For example gravity and  osmo appear to have a very close linear relationship. Collinearity between $x$ variables in linear regression models can cause trouble for statistical inference. Two correlated variables will compete for the ability to predict the response variable, leading to unstable estimates. This is not a problem for prediction of the response, if prediction is the end goal of the model. But if our objective is to discover how the variables relate to the response, we should avoid collinearity.

We can more formally estimate the correlation among these variables using the corrplot package.

```{r }
library("corrplot")
Cor = cor(dat)
corrplot(Cor, type="upper", method="ellipse", tl.pos="d")
corrplot(Cor, type="lower", method="number", col="black", 
         add=TRUE, diag=FALSE, tl.pos="n", cl.pos="n")
```


#### Variable selection


One primary goal of this analysis is to find out which variables are related to the presence of calcium oxalate crystals. This objective is often called "variable selection." We have already seen one way to do this: fit several models that include different sets of variables and see which one has the best DIC. Another way to do this is to use a linear model where the priors for the $\beta$ coefficients favor values near 0 (indicating a weak relationship). This way, the burden of establishing association lies with the data. If there is not a strong signal, we assume it doesn't exist.

Rather than tailoring a prior for each individual $\beta$ based on the scale its covariate takes values on, it is customary to subtract the mean and divide by the standard deviation for each variable.

```{r }
# we scale the data by center / std
# we should exclude categorical variable from this operation
X = scale(dat[,-1], center=TRUE, scale=TRUE)
head(X[,"gravity"])
```

```{r }
# we can see all results all close to 0
colMeans(X)
```


```{r }
apply(X, 2, sd)
```
covariates are standard and scaled we can talk about the new prior (double exponential or laplace prior)

#### Variable selection

Our prior for the $\beta$ (which we'll call $b$ in the model) coefficients will be the double exponential (or Laplace) distribution, which as the name implies, is the exponential distribution with tails extending in the positive direction as well as the negative direction, with a sharp peak at 0. We can read more about it in the JAGS manual. The distribution looks like:

```{r }
ddexp = function(x, mu, tau) {
  0.5*tau*exp(-tau*abs(x-mu)) 
}
curve(ddexp(x, mu=0.0, tau=1.0), from=-5.0, to=5.0, ylab="density", main="Double exponential\ndistribution") # double exponential distribution
curve(dnorm(x, mean=0.0, sd=1.0), from=-5.0, to=5.0, lty=2, add=TRUE) # normal distribution
legend("topright", legend=c("double exponential", "normal"), lty=c(1,2), bty="n")
```

```{r }
library("rjags")
```


```{r }
mod1_string = " model {
    for (i in 1:length(y)) {

        # likelihood comes from a bernouilli distrib
        # proba of success of getting a 1 is p[i]
        y[i] ~ dbern(p[i])

        # logit of p, b = beta
        logit(p[i]) = int + b[1]*gravity[i] + b[2]*ph[i] + b[3]*osmo[i] + b[4]*cond[i] + b[5]*urea[i] + b[6]*calc[i]
    }

    # non informative prior normal strong variance
    int ~ dnorm(0.0, 1.0/25.0)

    # we create the prior for the individual beta prior
    for (j in 1:6) {
        b[j] ~ ddexp(0.0, sqrt(2.0)) # has variance 1.0
    }
} "

set.seed(92)
head(X)

data_jags = list(y=dat$r, gravity=X[,"gravity"], ph=X[,"ph"], osmo=X[,"osmo"], cond=X[,"cond"], urea=X[,"urea"], calc=X[,"calc"])

params = c("int", "b")

mod1 = jags.model(textConnection(mod1_string), data=data_jags, n.chains=3)
update(mod1, 1e3)

mod1_sim = coda.samples(model=mod1,
                        variable.names=params,
                        n.iter=5e3)
mod1_csim = as.mcmc(do.call(rbind, mod1_sim))

## convergence diagnostics
plot(mod1_sim, ask=TRUE)

gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
autocorr.plot(mod1_sim)
effectiveSize(mod1_sim)

## calculate DIC
dic1 = dic.samples(mod1, n.iter=1e3)
```

Let's look at the results.

```{r }
summary(mod1_sim)
```
the par, inside of a numerical of our distribution, let's create the marginal distrib plot for ou 6 beta coefficient
```{r }
par(mfrow=c(3,2))
densplot(mod1_csim[,1:6], xlim=c(-3.0, 3.0))
```
we print the colname to remember that the first one is related to b1 second one to b2 etc.
```{r }
colnames(X) # variable names
```


It is clear that the coefficients for variables gravity, cond (conductivity), and calc (calcium concentration) are not 0(beta1, beta4, beta6) because we can notice that 0 is almost not contained in the density. The posterior distribution for the coefficient of osmo (osmolarity) looks like the prior, and is almost centered on 0 still, so we'll conclude that osmo is not a strong predictor of calcium oxalate crystals. The same goes for ph. (they contain 0 and with double exponential close to 0 is not good)

urea (urea concentration) (Beta5) appears to be a borderline case. However, if we refer back to our correlations among the variables, we see that urea is highly correlated with gravity, so we are going to remove this variable.

Our second model looks like this:
we removed the variable not interesting/ correlation etc.
```{r }
mod2_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dbern(p[i])
        logit(p[i]) = int + b[1]*gravity[i] + b[2]*cond[i] + b[3]*calc[i]
    }
    int ~ dnorm(0.0, 1.0/25.0)
    for (j in 1:3) {
      # we switch the prior
        b[j] ~ dnorm(0.0, 1.0/25.0) # noninformative for logistic regression
    }
} "

mod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)
```



```{r }
update(mod2, 1e3)

mod2_sim = coda.samples(model=mod2,
                        variable.names=params,
                        n.iter=5e3)
mod2_csim = as.mcmc(do.call(rbind, mod2_sim))

plot(mod2_sim, ask=TRUE)

gelman.diag(mod2_sim)
autocorr.diag(mod2_sim)
autocorr.plot(mod2_sim)
effectiveSize(mod2_sim)

dic2 = dic.samples(mod2, n.iter=1e3)
```


#### Results



```{r }
dic1
```


```{r }
dic2
```


```{r }
summary(mod2_sim)
```
Commentary summary:
we have a positive coefficient gravity is associated with an increase in the probability of observing crystals.
with a negative coefficient the conductivity is associated with a decrease of observing crystal
and calcium concentration with an increase in probability of calcium 

All the posterior mean are fairly far from zero (look at the SD + mean) so we would say they are significant constributors to the probability of calcium

```{r }
colMeans(mod1_csim)
```

We can compare the two models, on the first model, b1 =gravity was positive they are similar. The fourth beta conductivity negative too and calcium b6 close to our new results.

```{r }
HPDinterval(mod2_csim)
```


```{r }
par(mfrow=c(3,1))
densplot(mod2_csim[,1:3], xlim=c(-3.0, 3.0))
```

```{r }
colnames(X)[c(1,4,6)] # variable names
```

The DIC is actually better for the first model. Note that we did change the prior between models, and generally we should not use the DIC to choose between priors. Hence comparing DIC between these two models may not be a fair comparison. Nevertheless, they both yield essentially the same conclusions. Higher values of gravity and calc (calcium concentration) are associated with higher probabilities of calcium oxalate crystals, while higher values of cond (conductivity) are associated with lower probabilities of calcium oxalate crystals.

There are more modeling options in this scenario, perhaps including transformations of variables, different priors, and interactions between the predictors, but we'll leave it to you to see if you can improve the model.


### Lesson 9.3

#### Prediction from a logisic regression model

How do we turn model parameter estimates into model predictions? The key is the form of the model. Remember that the likelihood is Bernoulli, which is 1 with probability pp. We modeled the logit of pp as a linear model, which we showed in the first segment of this lesson leads to an exponential form for $E(y)=p(y)=p$.

Take the output from our model in the last segment. We will use the posterior means as point estimates of the parameters.

```{r }
(pm_coef = colMeans(mod2_csim))
```

The posterior mean of the intercept was about $-0.15$. Since we centered and scaled all of the covariates, values of $0$ for each $x$ correspond to the average values. Therefore, if we use our last model, then our point estimate for the probability of calcium oxalate crystals when gravity, cond, and calc are at their average values is $1/(1+ e^{-(-0.15))})=4625702$

Now suppose we want to make a prediction for a new specimen whose value of gravity is average, whose value of cond is one standard deviation below the mean, and whose value of calc is one standard deviation above the mean. Our point estimate for the probability of calcium oxalate crystals is $1/(1+ e^{-(-0.15+1.4*0.0-1.3*(-1.0)+1.9*(1.0))}=0.9547825$

If we want to make predictions in terms of the original $x$ variable values, we have two options:





    1.For each $x$ variable, subtract the mean and divide by the standard deviation for that variable in the original data set used to fit the model.  
    2.Re-fit the model without centering and scaling the covariates.
    
    
    
    
    
    

#### Predictive checks

We can use the same ideas to make predictions for each of the original data points. This is similar to what we did to calculate residuals with earlier models.

First we take the $X$ matrix and matrix multiply it with the posterior means of the coefficients. Then we need to pass these linear values through the inverse of the link function as we did above.

```{r }
pm_Xb = pm_coef["int"] + X[,c(1,4,6)] %*% pm_coef[1:3]
phat = 1.0 / (1.0 + exp(-pm_Xb))
head(phat)
```


These phat values are the model's predicted probability of calcium oxalate crystals for each data point. We can get a rough idea of how successful the model is by plotting these predicted values against the actual outcome.


```{r }
plot(phat, jitter(dat$r))
```

Suppose we choose a cutoff for these predicted probabilities. If the model tells us the probability is higher than $0.5$, we will classify the observation as a $1$ and if it is less than 0.5, we will classify it as a $0$. That way the model classifies each data point. Now we can tabulate these classifications against the truth to see how well the model predicts the original data.

```{r }
(tab0.5 = table(phat > 0.5, data_jags$y))
```


```{r }
sum(diag(tab0.5)) / sum(tab0.5)
```

The correct classification rate is about 76%, not too bad, but not great.

Now suppose that it is considered really bad to predict no calcium oxalate crystal when there in fact is one. We might then choose to lower our threshold for classifying data points as 1s. Say we change it to $0.3$. That is, if the model says the probability is greater than $0.3$, we will classify it as having a calcium oxalate crystal.

```{r }
(tab0.3 = table(phat > 0.3, data_jags$y))
```

```{r }
sum(diag(tab0.3)) / sum(tab0.3)
```

It looks like we gave up a little classification accuracy, but we did indeed increase our chances of detecting a true positive.

We could repeat this exercise for many thresholds between $0$ and $1$, and each time calculate our error rates. This is equivalent to calculating what is called the ROC (receiver-operating characteristic) curve, which is often used to evaluate classification techniques.

These classification tables we have calculated were all in-sample. They were predicting for the same data used to fit the model. We could get a less biased assessment of how well our model performs if we calculated these tables for data that were not used to fit the model. For example, before fitting the model, you could withhold a set of randomly selected "test" data points, and use the model fit to the rest of the "training" data to make predictions on your "test" set.

### Quizz :

```{r }
library("MASS")
data("OME")
#?OME # background on the data
head(OME)

any(is.na(OME)) # check for missing values
dat <- subset(OME, OME != "N/A") # manually remove OME missing values identified with "N/A"
dat$OME <- factor(dat$OME)
str(dat)

plot(dat$Age, dat$Correct / dat$Trials)
plot(dat$OME, dat$Correct / dat$Trials)
plot(dat$Loud, dat$Correct / dat$Trials)
plot(dat$Noise, dat$Correct / dat$Trials)
```

```{r }
mod_glm <- glm(Correct/Trials ~ Age + OME + Loud + Noise, data=dat, weights=Trials, family="binomial")
summary(mod_glm)
plot(residuals(mod_glm, type="deviance"))
plot(fitted(mod_glm), dat$Correct/dat$Trials)
```

```{r }
mod_glm
```


```{r }
X <- model.matrix(mod_glm)[,1] # -1 removes the column of 1s for the intercept
head(X)

```



```{r }
# 
# mod_string = " model {
# 	for (i in 1:length(y)) {
# 		y[i] ~ dbin(phi[i], n[i])
# 		logit(phi[i]) = b0 + b[1]*Age[i] + b[2]*OMElow[i] + b[3]*Loud[i] + b[4]*Noiseincoherent[i]
# 	}
# 	
# 	b0 ~ dnorm(0.0, 1.0/5.0^2)
# 	for (j in 1:4) {
# 		b[j] ~ dnorm(0.0, 1.0/4.0^2)
# 	}
# 	
# } "
# 
# data_jags = as.list(as.data.frame(X))
# data_jags$y = dat$Correct # this will not work if there are missing values in dat (because they would be ignored by model.matrix). Always make sure that the data are accurately pre-processed for JAGS.
# data_jags$n = dat$Trials
# str(data_jags) # make sure that all variables have the same number of observations (712).
# 
# 
# 
# data_jags = list(y=dat$Correct, Age=dat[,"Age"], OMElow=dat[,"OMElow"], Loud=dat[,"Loud"], Noiseincoherent=dat[,"Noiseincoherent"])
# 
# params = c("int", "b")
# 
# mod1 = jags.model(textConnection(mod1_string), data=data_jags, n.chains=3)
# update(mod1, 1e3)
# 
# mod1_sim = coda.samples(model=mod1,
#                         variable.names=params,
#                         n.iter=5e3)
# mod1_csim = as.mcmc(do.call(rbind, mod1_sim))
# 
# ## convergence diagnostics
# plot(mod1_sim, ask=TRUE)
# 
# gelman.diag(mod1_sim)
# autocorr.diag(mod1_sim)
# autocorr.plot(mod1_sim)
# effectiveSize(mod1_sim)
# 
# ## calculate DIC
# dic1 = dic.samples(mod1, n.iter=1e3)

```

```{r }
# (tab0.7 = table(phat > 0.7, (dat$Correct / dat$Trials) > 0.7))
# sum(diag(tab0.7)) / sum(tab0.7)
```

